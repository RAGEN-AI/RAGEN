# webshop report

## Experiemnt setup
We conducted our experiments using 2 model sizes: Qwen2.5-0.5B-Instruct, Qwen2.5-3B-Instruct and 4 STARPO variants: starpo-s grpo, starpo-s ppo, starpo grpo, starpo ppo. For starpo we use 2 groups of size 16, for starpo-s we use 8 groups of size 16 and filter rate 0.25.

Due to resource constraints, we trained 200 steps for each model with a max trajectory length of 9.

## Results
None of the 0.5 models were able to get stablized positive training reward. For 3b models we got the task score (normalized to 0-1) on test set:

| training variant | starting test score | best test score |
|------------------|---------------------|-----------------|
| starpo-ppo       | 0.09                | 0.23            |
| starpo-grpo      | 0.07                | 0.36            |
| starpos-ppo      | 0.09                | 0.40            |
| starpos-grpo     | 0.09                | 0.45            |

## Observations and takeaways

### model size
Larger base model is better for schematic-rich task like webshop. We weren't be able to get any training reward for 0.5b model. We get reasonable training performance for 3b model. Preliminary results show that 7b model is better than 3b, despite that we haven't got complete training results for any 7b model yet due to prioritization and resource constraints.

### training scheme
Unlike tasks like sokoban, vanilla starpo works for webshop - no collapse has happened. That said, starpo-s remains superior to starpo like other tasks we have evaluated so far.

Within the same starpo scheme, GRPO performed better than PPO. We suspect that long-context task like webshop is more challenging for PPO with the extra critic model.

### prompting
Webshop is more sensitive to prompt than other tasks we've evaluated so far. Our current prompt is not really optimal, but it lifed the initial performance of 3b models. We believe this phenomenon is due to the fact that webshop is complicated, schematic-rich task which requires more reasoning ability from the base model. With larger base models, the affect of prompting should be less prominent.

### max actions
We observed that max number of actions is correlated with the success rate. We chose 9 actions for 3b models as a balance between performance and memory consumption. Allowing more actions may help/hurt performance.

### perculiar behavior of starpo-s ppo
Starpo-s ppo is the only strategy that has a U shaped reponse length curve - first decrease then increase. All others' reponse length are almost monotonically decreasing.

Looking at the training behavior, we found at the beginning of training, the model often behaves echo trap when choosing products - it ponders on the product or search page and doesn't buy anything. Over time, the model learns to buy a product within the turn limit, which results in a decrease in response length. Nevertheless, to achieve a higher reward, the model realizes to choose more options, which was reflected in an increase in response length.

The behavior is confirmed by r_options going up, success purchase going down. All non-zero reward went up. 
This means the model is actually learning to improve the purchase quality. This is encouraging, but we need to further understand why this only happens in starpo-s ppo.

### failure modes
There're still some failure modes the model needs to learn after 200 steps, like:
- over confidence in buying the wrong product
- echoing when choosing options

### limitations and next steps
If we compare the trajectory generated by a model against the optimal trajectory by human expert, it's notable that the model is under explored on search phase and over explored on purchase phase. It's very hard for the model to learn exploring the product pages meticulously and extensively while being precise and decisive on the purchase phase. This requires the rollout sample generation to be smart enough in balancing between exploration and exploitation across different turns. This could mean a more sophisticated reward modeling and/or a bigger model. We need to at least evaluate on 7b models - which is the smallest model size evaluated on webshop in other works.
