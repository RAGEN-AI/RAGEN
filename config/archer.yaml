defaults:
  - base

# ArCHer-specific configuration
# Based on "ArCHer: Training Language Model Agents via Hierarchical Multi-Turn RL"
# by Yifei Zhou et al. (ICML 2024)

trainer:
  experiment_name: archer_baseline

# Use ArCHer advantage estimator
algorithm:
  adv_estimator: archer
  gamma: 0.95  # Discount factor (matching ArCHer original)
  lam: 1.0
  high_level_gamma: 0.95  # High-level discount factor for turn-level rewards
  
# ArCHer works best with multi-turn environments
agent_proxy:
  max_turn: 10  # Allow more turns for ArCHer to demonstrate hierarchical learning
  use_turn_scores: True  # Important for ArCHer's hierarchical structure
  
# ArCHer parameters (matching original implementation)
archer:
  alpha: 0.1  # Mixing coefficient between high-level and low-level value functions
  tau: 0.1    # Soft update parameter for target networks (ArCHer original uses 0.1)
  enabled: true  # Flag to enable ArCHer critic
  
# ArCHer-specific training configuration
actor_rollout_ref:
  actor:
    ppo_epochs: 3  # Actor epochs (matching ArCHer original)
    
critic:
  ppo_epochs: 3  # Critic epochs (reduced from ArCHer's 50 for faster testing)
  optim:
    lr: 1e-5  # Critic learning rate (matching ArCHer original)
    
# Use environments that benefit from hierarchical reasoning
es_manager:
  train:
    env_groups: 8
    group_size: 16
    env_configs:
      tags: ["SimpleSokoban", "FrozenLake"]  # Multi-step reasoning tasks
      n_groups: [4, 4]
  val:
    env_groups: 256
    group_size: 1
    env_configs:
      tags: ["SimpleSokoban", "FrozenLake"]
      n_groups: [128, 128]