dataset:
  data_path: '$ALFWORLD_DATA/json_2.1.1/train'
  eval_id_data_path: '$ALFWORLD_DATA/json_2.1.1/valid_seen'    # null/None to disable
  eval_ood_data_path: '$ALFWORLD_DATA/json_2.1.1/valid_unseen' # null/None to disable
  num_train_games: 1                                          # reduced from -1 to 10 for quick testing
  num_eval_games: 1                                            # reduced from -1 to 5 for quick testing

logic:
  domain: '$ALFWORLD_DATA/logic/alfred.pddl'                   # PDDL domain file that defines the world dynamics
  grammar: '$ALFWORLD_DATA/logic/alfred.twl2'                  # Grammar file that defines the text feedbacks

env:
  type: 'AlfredTWEnv'                                          # 'AlfredTWEnv' or 'AlfredThorEnv' or 'AlfredHybrid'
  domain_randomization: False                                  # shuffle Textworld print order and object id nums
  task_types: [1]                                              # reduced to just task type 1 (simplest)
  expert_timeout_steps: 10                                     # reduced from 20 to 10
  expert_type: "handcoded"                                     # 'handcoded' or 'planner'. Note: the planner is very slow for real-time use
  goal_desc_human_anns_prob: 0.0                               # prob of using human-annotated goal language instead of templated goals

  hybrid:
    start_eps: 100                                             # reduced from 100000 to 100
    thor_prob: 0.5                                             # prob of AlfredThorEnv during hybrid training
    eval_mode: "tw"                                            # 'tw' or 'thor' - env used for evaluation during hybrid training

  thor:
    screen_width: 150                                          # reduced from 300 to 150
    screen_height: 150                                         # reduced from 300 to 150
    smooth_nav: False                                          # smooth rotations, looks, and translations during navigation (very slow)
    save_frames_to_disk: False                                 # save frame PNGs to disk (useful for making videos)
    save_frames_path: './videos/'                              # path to save frame PNGs

controller:
  type: 'oracle'                                               # 'oracle' or 'oracle_astar' or 'mrcnn' or 'mrcnn_astar' (aka BUTLER)
  debug: False
  load_receps: True                                            # load receptacle locations from precomputed dict (if available)

mask_rcnn:
  pretrained_model_path: '$ALFWORLD_DATA/detectors/mrcnn.pth'

general:
  random_seed: 42
  use_cuda: True                                               # disable this when running on machine without cuda
  visdom: False                                                # plot training/eval curves, run with visdom server
  task: 'alfred'
  training_method: 'dagger'                                    # 'dqn' or 'dagger'
  save_path: './training/'                                     # path to save pytorch models
  observation_pool_capacity: 1                                 # reduced from 3 to 1
  hide_init_receptacles: False                                 # remove initial observation containing navigable receptacles

  training:
    batch_size: 1                                              # reduced from 2 to 1
    max_episode: 10                                            # reduced from 100 to 10
    smoothing_eps: 0.1
    optimizer:
      learning_rate: 0.001
      clip_grad_norm: 5

  evaluate:
    run_eval: True
    batch_size: 2                                              # reduced from 10 to 2
    env:
      type: "AlfredTWEnv"

  checkpoint:
    report_frequency: 10                                       # reduced from 1000 to 10
    experiment_tag: 'test_lite'                                # changed to indicate lite version
    load_pretrained: False                                     # during test, enable this so that the agent load your pretrained model
    load_from_tag: 'not loading anything'                      # name of pre-trained model to load in save_path

  model:
    encoder_layers: 1
    decoder_layers: 1
    encoder_conv_num: 1                                        # reduced from 3 to 1
    block_hidden_dim: 16                                       # reduced from 32 to 16
    n_heads: 1
    dropout: 0.1
    block_dropout: 0.1
    recurrent: True

rl:
  action_space: "admissible"                                  # 'admissible' (candidates from text engine) or 'generation' (seq2seq-style generation) or 'beam_search_choice' or 'exhaustive' (not working)
  max_target_length: 5                                        # reduced from 10 to 5
  beam_width: 1                                               # reduced from 3 to 1 (greedy)
  generate_top_k: 1                                           # reduced from 2 to 1

  training:
    max_nb_steps_per_episode: 10                              # reduced from 20 to 10
    learn_start_from_this_episode: 0                          # delay updates until this epsiode
    target_net_update_frequency: 10                           # reduced from 50 to 10

  replay:
    accumulate_reward_from_final: True
    count_reward_lambda: 0.0                                  # 0 to disable
    novel_object_reward_lambda: 0.0                           # 0 to disable
    discount_gamma_game_reward: 0.9
    discount_gamma_count_reward: 0.5
    discount_gamma_novel_object_reward: 0.5
    replay_memory_capacity: 100                               # reduced from 1000 to 100
    replay_memory_priority_fraction: 0.5
    update_per_k_game_steps: 1                                # reduced from 2 to 1
    replay_batch_size: 2                                      # reduced from 8 to 2
    multi_step: 1                                             # reduced from 2 to 1
    replay_sample_history_length: 1                           # reduced from 2 to 1
    replay_sample_update_from: 1                              # kept at 1

  epsilon_greedy:
    noisy_net: False                                          # if this is true, then epsilon greedy is disabled
    epsilon_anneal_episodes: 5                                # reduced from 100 to 5
    epsilon_anneal_from: 0.3
    epsilon_anneal_to: 0.1

dagger:
  action_space: "generation"                                  # 'admissible' (candidates from text engine) or 'generation' (seq2seq-style generation) or 'exhaustive' (not working)
  max_target_length: 5                                        # reduced from 10 to 5
  beam_width: 1                                               # reduced from 3 to 1 (greedy)
  generate_top_k: 1                                           # reduced from 2 to 1
  unstick_by_beam_search: False                               # use beam-search for failed actions, set True during evaluation

  training:
    max_nb_steps_per_episode: 10                              # reduced from 20 to 10

  fraction_assist:
    fraction_assist_anneal_episodes: 5                        # reduced from 100 to 5
    fraction_assist_anneal_from: 1.0
    fraction_assist_anneal_to: 0.01

  fraction_random:
    fraction_random_anneal_episodes: 0
    fraction_random_anneal_from: 0.0
    fraction_random_anneal_to: 0.0

  replay:
    replay_memory_capacity: 100                               # reduced from 1000 to 100
    update_per_k_game_steps: 1                                # reduced from 2 to 1
    replay_batch_size: 2                                      # reduced from 8 to 2
    replay_sample_history_length: 1                           # reduced from 2 to 1
    replay_sample_update_from: 1                              # kept at 1

vision_dagger:
  model_type: "resnet"                                        # 'resnet' (whole image features) or 'maskrcnn_whole' (whole image MaskRCNN feats) or 'maskrcnn' (top k MaskRCNN detection feats) or 'no_vision' (zero vision input)
  resnet_fc_dim: 16                                           # reduced from 32 to 16
  maskrcnn_top_k_boxes: 2                                     # reduced from 5 to 2
  use_exploration_frame_feats: False                          # append feats from initial exploration (memory intensive!)
  sequence_aggregation_method: "average"                      # 'sum' or 'average' or 'rnn'
